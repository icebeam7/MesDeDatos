{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0447598f",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e1203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61ccd4",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2230045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.read_csv(\"Dataset.csv\")\n",
    "\n",
    "# importing an array of features\n",
    "x = Dataset.iloc[:, :-1].values \n",
    "# importing an array of dependent variable\n",
    "y = Dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6d6ef",
   "metadata": {},
   "source": [
    "We specified two variables, x for the features and y for the dependent variable. \n",
    "\n",
    "The features set consists of all rows and columns of our dataset except the last column. \n",
    "\n",
    "Similarly, the dependent variable y consists of all rows but only the last column.\n",
    "\n",
    "Let’s have a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6b6597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x) # returns an array of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e551f",
   "metadata": {},
   "source": [
    "Our data is imported successfully in the form of an array of features x and a dependent variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90687cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No ' 'Yes' 'No ' 'No ' 'Yes' 'Yes' 'No ' 'Yes' 'No ' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y) # viewing an array of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc19362",
   "metadata": {},
   "source": [
    "# Taking care of the missing data\n",
    "Missing data is a common problem that occurs when a dataset has no value for a feature in an observation (quite common in surveys).\n",
    "\n",
    "Missing data is harmful to machine learning models and requires appropriate handling.\n",
    "\n",
    "There are several techniques we use to handle the missing data:\n",
    "- Deleting the observation with the missing value(s)\n",
    "- Mean Imputation\n",
    "- Hot Deck Imputation\n",
    "- Cold Deck Imputation\n",
    "- Regression Imputation (Deterministic / Stochastic)\n",
    "\n",
    "Let's use mean imputation to handle missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b95fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the class called SimpleImputer from impute model in sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To replace the missing value we create below object of SimpleImputer class\n",
    "imputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Using the fit method, we apply the `imputa` object on the matrix of our feature x.\n",
    "# The `fit()` method identifies the missing values and computes the mean of such feature a missing value is present.\n",
    "\n",
    "imputa.fit(x[:, 1:3])\n",
    "\n",
    "# Repalcing the missing value using transform method\n",
    "x[:, 1:3] = imputa.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc8a3f",
   "metadata": {},
   "source": [
    "We obtain a matrix of features with the missing values replaced. The missing values on the Age and Salary columns are replaced with their respective column means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a54d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23d8c5",
   "metadata": {},
   "source": [
    "# Encoding categorical data\n",
    "\n",
    "During encoding, we transform text data into numeric data. Encoding categorical data involves changing data that fall into categories to numeric data.\n",
    "\n",
    "The Country and the Purchased columns of our dataset contain data that fall into categories. Since machine learning models are based on a mathematical equation, which takes only numerical inputs, it is challenging to compute the correlation between the feature and the dependent variables. To ensure this does not happen, we need to convert the string entries in the dataset into numbers.\n",
    "\n",
    "For our dataset, we shall encode France into 0, Spain into 1, and Germany into 2. However, our future machine learning model interprets the numerical order between 0 for France, 1 for Spain, and 2 for Germany do matter, which is not the case. To ensure this misinterpretation does not occur, we make use of one-hot encoding.\n",
    "\n",
    "One-hot encoding converts our categorical Country column into three columns. It creates a unique binary vector for each country such that there is no numerical order between the country categories.\n",
    "\n",
    "Let’s see how One-hot encoding enables us to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d9f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder= 'passthrough')\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d9f69ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def51e2e",
   "metadata": {},
   "source": [
    "From the output, the Country column has been transformed into 3 columns with each row representing only one encoded column where, France was encoded into a vector [1.0 0.0 0.0], Spain encoded into vector [0.0 0.0 1.0], and Germany encoded into vector [0.0 1.0 0.0] where they’re all unique.\n",
    "\n",
    "Let's encode our depended variable y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2388a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b13bc",
   "metadata": {},
   "source": [
    "Our dependent variable is encoded successfully into 0’s and 1’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecbbb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e375a1",
   "metadata": {},
   "source": [
    "# Splitting the dataset into the training and test sets\n",
    "In machine learning, we split the dataset into a training set and a test set. The training set is the fraction of a dataset that we use to implement the model. On the other hand, the test set is the fraction of the dataset that we use to evaluate the performance our the model.\n",
    "\n",
    "The test set is assumed to be unknown during the process of the model implementation.\n",
    "\n",
    "We need to split our dataset into four subsets, x_train, x_test, y_train, and y_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b9b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f3f6a",
   "metadata": {},
   "source": [
    "Let’s print the output. Our dataset is successfully split. Our features set was divided into eight observations for the x_train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf56b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d0238",
   "metadata": {},
   "source": [
    "... and 2 for the x_test, which correspond (since we set our seed, random = 1) to the same splitting of the dependent variable y.\n",
    "\n",
    "The selection is made randomly, and it’s possible at any single execution to obtain different subsets other than the above output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e32ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7acea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa63d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47fda8",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "In most cases, we shall work with datasets whose features are not on the same scale. Some features often have tremendous values, and others have small values.\n",
    "\n",
    "Suppose we implement our machine learning model on such datasets. In that case, features with tremendous values dominate those with small values, and the machine learning model treats those with small values as if they don’t exist (their influence on the data is not be accounted for). To ensure this is not the case, we need to scale our features on the same range, i.e., within the interval of -3 and 3.\n",
    "\n",
    "Therefore, we shall only scale the Age and Salary columns of our x_train and x_test into this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b209c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# we only aply the feature scaling on the features other than dummy variables.\n",
    "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n",
    "x_test[:, 3:] = sc.fit_transform(x_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77176883",
   "metadata": {},
   "source": [
    "In the x_train and x_test, we only scaled the Age and Salary columns and not the dummy variables. This is because scaling the dummy variables may interfere with their intended interpretation even though they fall within the required range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45f4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f5051c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.0 -1.0]\n",
      " [1.0 0.0 0.0 1.0 1.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
